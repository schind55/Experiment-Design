{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from classes.Classifiers.Classifiers import Classifiers\n",
    "from classes.Classifiers.ClassBalancer import ClassBalancer\n",
    "from classes.Classifiers.FeatureSelector import FeatureSelector\n",
    "from classes.Classifiers.ModelComparison import ModelComparison\n",
    "from classes.Utils.PandasUtils import PandasUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from glob import glob \n",
    "\n",
    "#os related operations\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#data structures\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from itertools import islice\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_classif \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve  \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC,SVC #multiclass\n",
    "from sklearn.linear_model import LogisticRegression #multiclass\n",
    "from sklearn.ensemble import RandomForestClassifier #multilabel, multioutput\n",
    "from sklearn.tree import DecisionTreeClassifier  #multilabel, multioutput\n",
    "from sklearn.ensemble import GradientBoostingClassifier #multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "results_path = os.path.join(path,'results','single-label-classification','')\n",
    "features_path = os.path.join(path,'features','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are stored in a dict\n",
    "acc_default = {}\n",
    "f1score_default = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['code-stat_','code-extend-proper_','all-features_','_no-code','code_','code-comment_']\n",
    "content = 'code'\n",
    "setting = '_'+content+'_'+'scaled'\n",
    "settings = [m+setting for m in ['rf','dt','gb','lsvc','svc','log'] ]\n",
    "\n",
    "for model_name in settings:\n",
    "    acc_default[model_name] = {}\n",
    "    f1score_default[model_name] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename                0\n",
      "cell_type               0\n",
      "cell_number             0\n",
      "execution_count         0\n",
      "linesofcomment          0\n",
      "linesofcode             0\n",
      "variable_count          0\n",
      "function_count          0\n",
      "text/plain              0\n",
      "image/png               0\n",
      "text/html               0\n",
      "execute_result          0\n",
      "display_data            0\n",
      "stream                  0\n",
      "error                   0\n",
      "text                    0\n",
      "comment                 0\n",
      "code_line_before        0\n",
      "code_line_after         0\n",
      "markdown_heading        0\n",
      "packages_info           0\n",
      "primary_label           0\n",
      "helper_functions        0\n",
      "load_data               0\n",
      "data_exploration        0\n",
      "data_preprocessing      0\n",
      "evaluation              0\n",
      "modelling               0\n",
      "prediction              0\n",
      "result_visualization    0\n",
      "save_results            0\n",
      "comment_only            0\n",
      "dtype: int64\n",
      "(5833, 32)\n",
      "(1927, 32)\n",
      "(1918, 32)\n"
     ]
    }
   ],
   "source": [
    "test_features = pd.read_pickle(features_path+'test_features.pkl')\n",
    "train_features = pd.read_pickle(features_path+'train_features.pkl')\n",
    "validation_features = pd.read_pickle(features_path+'validation_features.pkl')\n",
    "#print(train_features.isna().sum())\n",
    "train_features.index = range(train_features.shape[0])\n",
    "validation_features.index = range(validation_features.shape[0])\n",
    "test_features.index = range(test_features.shape[0])\n",
    "\n",
    "train_features.fillna(0,inplace=True)\n",
    "test_features.fillna(0,inplace=True)\n",
    "validation_features.fillna(0,inplace=True)\n",
    "print(train_features.isna().sum())\n",
    "print(train_features.shape)\n",
    "print(validation_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['helper_functions','load_data',\n",
    "              'data_preprocessing','data_exploration',\n",
    "              'modelling','evaluation','prediction',\n",
    "              'result_visualization','save_results',\n",
    "              'comment_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training + validation dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "#df= df.add(features)\n",
    "#df = df.add(train_features)\n",
    "#df = df.add(validation_features)\n",
    "#df.shape\n",
    "df = pd.concat([df, train_features,  validation_features], axis=0)\n",
    "\n",
    "# Reset index if needed\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notebook in training+validation:  376\n",
      "Total notebook in test:  94\n"
     ]
    }
   ],
   "source": [
    "print(\"Total notebook in training+validation: \",len(set(df.filename.values)))\n",
    "print(\"Total notebook in test: \",len(set(test_features.filename.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Setup and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorisation of features \n",
    "*if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df):\n",
    "    cat = pd.Categorical(df['primary_label'].values, categories=labels)\n",
    "    y, uniques = pd.factorize(cat)\n",
    "    df['primary_label'] = np.asarray(y)\n",
    "    \n",
    "    cat = pd.Categorical(df['filename'].values)\n",
    "    files, uniques = pd.factorize(cat)\n",
    "    df['filename'] = np.asarray(files)\n",
    "    \n",
    "    return df,files,cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(7760, 32)\n",
      "[0 3 5 1 2 4 8 7 6 9]\n",
      "(1918, 32)\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "#temp_df = temp_df.add(df.copy())\n",
    "temp_df = pd.concat([df, temp_df, test_features], axis=0)\n",
    "\n",
    "# Reset index if needed\n",
    "temp_df = temp_df.reset_index(drop=True)\n",
    "#temp_df = temp_df.add(test_features.copy())\n",
    "#print(temp_df)\n",
    "temp_df.index = range(temp_df.shape[0])\n",
    "temp_df,cat,files = categorize(temp_df)\n",
    "#print(cat)\n",
    "#print(files)\n",
    "df1 = temp_df[0:df.shape[0]].copy()\n",
    "df1.index = range(df1.shape[0])\n",
    "print(df1.primary_label.unique())\n",
    "print(df1.shape)\n",
    "\n",
    "df2 = temp_df[df.shape[0]:].copy()\n",
    "df2.index = range(df2.shape[0])\n",
    "print(df2.primary_label.unique())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for a pre-defined split of validation set for crossfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = []\n",
    "for i in range(0,train_features.shape[0]):\n",
    "    test_fold.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5833"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = {}\n",
    "t_fold = 1\n",
    "count = 1\n",
    "for each in set(validation_features.filename.values):\n",
    "    folds[each] = t_fold \n",
    "    if count%10 == 0:\n",
    "        t_fold += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in validation_features.iterrows():\n",
    "    test_fold.append(folds[row['filename']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7760"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### set the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "print(set(test_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the classifier and indicate conditions to restrict the dataframe we will be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the restricted dataframe:  (7760, 32)\n",
      "Resetting index.\n",
      "train.shape,test.shape\n",
      "(7760, 32) (1918, 32)\n"
     ]
    }
   ],
   "source": [
    "model = Classifiers(df1,labels)\n",
    "\n",
    "conditions = (df1.cell_type == 'code')   #not needed because our dataframe already has only code datapoints\n",
    "model.apply_conditions_to_dataframe(conditions) #a new dataframe #df_restricted will be created internally\n",
    "\n",
    "(train,test,indices_train,indices_test)=model.test_train_data_set(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                0\n",
       "cell_type               0\n",
       "cell_number             0\n",
       "execution_count         0\n",
       "linesofcomment          0\n",
       "linesofcode             0\n",
       "variable_count          0\n",
       "function_count          0\n",
       "text/plain              0\n",
       "image/png               0\n",
       "text/html               0\n",
       "execute_result          0\n",
       "display_data            0\n",
       "stream                  0\n",
       "error                   0\n",
       "text                    0\n",
       "comment                 0\n",
       "code_line_before        0\n",
       "code_line_after         0\n",
       "markdown_heading        0\n",
       "packages_info           0\n",
       "primary_label           0\n",
       "helper_functions        0\n",
       "load_data               0\n",
       "data_exploration        0\n",
       "data_preprocessing      0\n",
       "evaluation              0\n",
       "modelling               0\n",
       "prediction              0\n",
       "result_visualization    0\n",
       "save_results            0\n",
       "comment_only            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helper_functions',\n",
       " 'load_data',\n",
       " 'data_preprocessing',\n",
       " 'data_exploration',\n",
       " 'modelling',\n",
       " 'evaluation',\n",
       " 'prediction',\n",
       " 'result_visualization',\n",
       " 'save_results',\n",
       " 'comment_only']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (Representation and Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the important feature is the code itself and other features (extended/contextual, comment, code statistics are considered supplementary features in our task) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new (lexical) feature column created as 'new_text'\n"
     ]
    }
   ],
   "source": [
    "#'text','comment','code_line_before','code_line_after', 'markdown_heading', 'packages_info'\n",
    "features = ['text']\n",
    "train,test = model.set_lexical_features(features) #same df is modified and contains our new feature column *new_text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply preprocessing to the text feature\n",
    "we will use code_text_processing function to process our lexical feature *new_text* we created previously and store them in *text_processed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = model.preprocessing('new_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(model)\n",
    "#print(model.train_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tfidf and chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf transformation finished. shape of the feature vector:  (7760, 48485) (1918, 48485)\n",
      "Selecting 1000 features...\n",
      "train,test shape\n",
      "(7760, 1000) (1918, 1000)\n"
     ]
    }
   ],
   "source": [
    "#vectorize the text (lexical) in 'text_processed' column\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),use_idf=True,max_df=0.2,min_df=2,stop_words='english') \n",
    "X_train,X_test,tfidf = model.vectorization(tfidf)\n",
    "\n",
    "#use feature selection if necessary (pass k and training label)\n",
    "k = 1000 #k lexical features to be retained\n",
    "X_train_features,X_test_features,selector = model.feature_selection(chi2,k,train.primary_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf features:  48485\n",
      "before feature selection:  48485\n",
      "after feature selection:  1000\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features = tfidf.get_feature_names()\n",
    "tfidf_features = tfidf.get_feature_names_out()\n",
    "print(\"tfidf features: \",len(tfidf_features))\n",
    "print(\"before feature selection: \",len(selector.get_support()))\n",
    "selected_features = selector.get_support()\n",
    "text_features = [tfidf_features[i] for i in range(len(tfidf.get_feature_names_out())) if selected_features[i]==True]\n",
    "print(\"after feature selection: \",len(text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking feature  linesofcomment   (7760, 1001)\n",
      "taking feature  linesofcode   (7760, 1002)\n",
      "taking feature  variable_count   (7760, 1003)\n",
      "taking feature  function_count   (7760, 1004)\n",
      "taking feature  linesofcomment   (1918, 1001)\n",
      "taking feature  linesofcode   (1918, 1002)\n",
      "taking feature  variable_count   (1918, 1003)\n",
      "taking feature  function_count   (1918, 1004)\n",
      "statistical features added\n"
     ]
    }
   ],
   "source": [
    "#following set of features are represented in numerical form\n",
    "#metric features are: ['linesofcomment','linesofcode','variable_count','function_count']\n",
    "#extended features are: ['filename','cell_number','execution_count','text/plain' , 'image/png', 'text/html', 'execute_result', 'display_data', 'stream', 'error']\n",
    "\n",
    "stat_features = ['linesofcomment','linesofcode','variable_count','function_count']\n",
    "X_train_features_,X_test_features_ = model.set_statistical_features(stat_features,X_train_features,X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Together, text_features and stat_features form the original feature vector of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = text_features+stat_features\n",
    "len(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the features \n",
    "since our statistical features are scaled in a different way to our text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_features_ = ss.fit_transform(X_train_features_)\n",
    "X_test_features_ = ss.transform(X_test_features_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare Classifiers \n",
    "*default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_code_scaled\n",
      "model:  rf 1000\n",
      " acc:  0.7106360792492179 f1score:  0.7029370860842276\n",
      "model:  dt 1000\n",
      " acc:  0.5714285714285714 f1score:  0.5957588972841119\n",
      "model:  gb 1000\n",
      " acc:  0.6934306569343066 f1score:  0.6848556784735591\n",
      "model:  lsvc 1000\n",
      " acc:  0.6548488008342023 f1score:  0.6423958034493278\n",
      "model:  svc 1000\n",
      " acc:  0.6595411887382691 f1score:  0.6494170709387062\n",
      "model:  log 1000\n",
      " acc:  0.6418143899895725 f1score:  0.6253374118206392\n"
     ]
    }
   ],
   "source": [
    "ks = [k] \n",
    "print(setting)\n",
    "for k in ks:\n",
    "    model_pipelines = [('rf',Pipeline([('clf', RandomForestClassifier(random_state=500))])),\n",
    "                       ('dt',Pipeline([('clf', DecisionTreeClassifier(random_state=500))])),\n",
    "                       ('gb',Pipeline([('clf', GradientBoostingClassifier(random_state=500))])),\n",
    "                       ('lsvc',Pipeline([('clf', LinearSVC(random_state=500))])),\n",
    "                       ('svc',Pipeline([('clf', SVC(random_state=500))])),\n",
    "                       ('log',Pipeline([('clf', LogisticRegression(random_state=500))])),\n",
    "                       ]\n",
    "    \n",
    "    for pipe in model_pipelines:\n",
    "        model_name = pipe[0]\n",
    "        pipeline = pipe[1]\n",
    "        print('model: ',model_name, k)\n",
    "\n",
    "        # Singlelabel classifier\n",
    "        model = OneVsRestClassifier(pipeline).fit(X_train_features_,train['primary_label'])\n",
    "        prediction = model.predict(X_test_features_)\n",
    "        acc = accuracy_score(test['primary_label'], prediction)\n",
    "        model_name = model_name+setting\n",
    "        acc_default[model_name][k] = acc\n",
    "        f1score = metrics.f1_score(test['primary_label'], prediction, average = 'weighted')\n",
    "        f1score_default[model_name][k] = f1score\n",
    "        print(' acc: ', acc, 'f1score: ', f1score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(f1score_default).to_pickle(results_path+'f1score_default.pkl')\n",
    "pd.DataFrame.from_dict(acc_default).to_pickle(results_path+'acc_default.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf = pd.DataFrame()\n",
    "f1f = pd.DataFrame()\n",
    "f1 = pd.read_pickle(results_path+'f1score_default.pkl')\n",
    "a1 = pd.read_pickle(results_path+'acc_default.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_default).max().max()\n",
    "pd.DataFrame(f1score_default).max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in ['code-stat_','code-extend-proper_','all-features_','_no-code','code_','code-comment_']:\n",
    "    cols = a1[a1.filter(like=each).columns]\n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        acf[col] = a1[col]\n",
    "\n",
    "for each in ['code-stat_','code-extend-proper_','all-features_','_no-code','code_','code-comment_']:\n",
    "    cols = f1[f1.filter(like=each).columns]\n",
    "    for col in cols:\n",
    "        f1f[col] = f1[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = f1f[f1f >= f1f.max().max()].to_numpy()\n",
    "for idrow, row in enumerate(my_data):\n",
    "    for idcol, col in enumerate(row):\n",
    "        if not pd.isnull(col):\n",
    "            print(\"f1 Value :\"+str(col)+\" column:\"+str(idcol)+\" row:\"+str(idrow),f1f.columns[idcol],f1f.index[idrow])\n",
    "            \n",
    "my_data = acf[acf >= acf.max().max()].to_numpy()\n",
    "for idrow, row in enumerate(my_data):\n",
    "    for idcol, col in enumerate(row):\n",
    "        if not pd.isnull(col):\n",
    "            print(\"acc Value :\"+str(col)+\" column:\"+str(idcol)+\" row:\"+str(idrow),acf.columns[idcol],acf.index[idrow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score_dict, acc_dict = {},{}\n",
    "\n",
    "for col in f1f.T[1000].index.values:\n",
    "    f1score_dict[col] = round(f1f[col][1000],3)\n",
    "    acc_dict[col] = round(acf[col][1000],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slabel = pd.DataFrame({'model':f1score_dict.keys(),'accuracy':acc_dict.values(),'f1score':f1score_dict.values()})\n",
    "slabel.index = slabel['model']\n",
    "slabel.to_pickle(results_path+'single_label_results.pkl')\n",
    "slabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(train[\"primary_label\"], return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "import pickle\n",
    "file_path = 'C:\\\\Users\\\\lukas\\\\OneDrive\\\\Dokumente\\\\tuwien\\\\MachineLearning\\\\exdds\\\\results\\\\single-label-classification\\\\acc_default.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    loaded_object = pickle.load(file)\n",
    "\n",
    "print(loaded_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Best parameters using GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "accuracy_all = {}\n",
    "f1_score_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameters = {'rf':{}}\n",
    "parameters['rf']['estimator__rf__criterion'] = ['gini','entropy']\n",
    "parameters['rf']['estimator__rf__n_estimators'] = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "parameters['rf']['estimator__rf__class_weight']= ['balanced', 'balanced_subsample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "print(\"******\",k,\"*******\")\n",
    "model_pipelines = [('rf',Pipeline([('rf', RandomForestClassifier(random_state=500))]))]\n",
    "    \n",
    "for pipe in model_pipelines:\n",
    "    model_name = pipe[0]+'['+content+']'\n",
    "    pipeline = pipe[1]\n",
    "    print('model: ',model_name, ' :: ', 'pipeline: ', pipeline)\n",
    "           \n",
    "    CV = GridSearchCV(OneVsRestClassifier(pipeline), parameters[pipe[0]], scoring = 'accuracy', n_jobs= 1,cv=ps)\n",
    "    CV.fit(X_train_features_,train['primary_label'])\n",
    "    prediction_single = CV.predict(X_test_features_)\n",
    "    print(' acc: ', accuracy_score(test['primary_label'], prediction_single))\n",
    "    print(' f1score: ', metrics.f1_score(test['primary_label'], prediction_single, average = 'weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test['primary_label'], prediction_single, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "sns.heatmap(confusion_matrix(test['primary_label'], prediction_single)\n",
    "                    , annot=True, fmt='d',\n",
    "            cmap=sns.diverging_palette(20, 220, n=200),\n",
    "            linewidths = 1,robust=True, center=0)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "title = 'Confusion matrix - Single label RF Classifier'\n",
    "#ax.set_title(title,fontsize=15,fontweight='bold')\n",
    "ax.set_yticklabels(labels,rotation=0)\n",
    "ax.set_xticklabels(labels,rotation=90)\n",
    "fig.tight_layout()\n",
    "ax.figure.savefig(results_path+'singlelabelconfusion.eps', format='eps')\n",
    "print(\"results saved in \",results_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'code-comment'\n",
    "setting = '_'+content+'_'+'scaled'\n",
    "settings = [m+setting for m in ['rf','dt','gb','lsvc','svc','log'] ]\n",
    "\n",
    "for model_name in settings:\n",
    "    acc_default[model_name] = {}\n",
    "    f1score_default[model_name] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the restricted dataframe:  (7760, 32)\n",
      "Resetting index.\n",
      "train.shape,test.shape\n",
      "(7760, 32) (1918, 34)\n"
     ]
    }
   ],
   "source": [
    "model = Classifiers(df1,labels)\n",
    "\n",
    "conditions = (df1.cell_type == 'code')   #not needed because our dataframe already has only code datapoints\n",
    "model.apply_conditions_to_dataframe(conditions) #a new dataframe #df_restricted will be created internally\n",
    "\n",
    "(train,test,indices_train,indices_test)=model.test_train_data_set(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new (lexical) feature column created as 'new_text'\n"
     ]
    }
   ],
   "source": [
    "features = ['text', 'comment']\n",
    "train,test = model.set_lexical_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = model.preprocessing('new_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf transformation finished. shape of the feature vector:  (7760, 56123) (1918, 56123)\n",
      "Selecting 1000 features...\n",
      "train,test shape\n",
      "(7760, 1000) (1918, 1000)\n"
     ]
    }
   ],
   "source": [
    "#vectorize the text (lexical) in 'text_processed' column\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),use_idf=True,max_df=0.2,min_df=2,stop_words='english') \n",
    "X_train,X_test,tfidf = model.vectorization(tfidf)\n",
    "\n",
    "#use feature selection if necessary (pass k and training label)\n",
    "k = 1000 #k lexical features to be retained\n",
    "X_train_features,X_test_features,selector = model.feature_selection(chi2,k,train.primary_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf features:  56123\n",
      "before feature selection:  56123\n",
      "after feature selection:  1000\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features = tfidf.get_feature_names()\n",
    "tfidf_features = tfidf.get_feature_names_out()\n",
    "print(\"tfidf features: \",len(tfidf_features))\n",
    "print(\"before feature selection: \",len(selector.get_support()))\n",
    "selected_features = selector.get_support()\n",
    "text_features = [tfidf_features[i] for i in range(len(tfidf.get_feature_names_out())) if selected_features[i]==True]\n",
    "print(\"after feature selection: \",len(text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking feature  linesofcomment   (7760, 1001)\n",
      "taking feature  linesofcode   (7760, 1002)\n",
      "taking feature  variable_count   (7760, 1003)\n",
      "taking feature  function_count   (7760, 1004)\n",
      "taking feature  linesofcomment   (1918, 1001)\n",
      "taking feature  linesofcode   (1918, 1002)\n",
      "taking feature  variable_count   (1918, 1003)\n",
      "taking feature  function_count   (1918, 1004)\n",
      "statistical features added\n"
     ]
    }
   ],
   "source": [
    "stat_features = ['linesofcomment','linesofcode','variable_count','function_count']\n",
    "X_train_features_,X_test_features_ = model.set_statistical_features(stat_features,X_train_features,X_test_features)\n",
    "feature_vector = text_features+stat_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_ = ss.fit_transform(X_train_features_)\n",
    "X_test_features_ = ss.transform(X_test_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_code_scaled\n",
      "model:  rf 1000\n",
      " acc:  0.700208550573514 f1score:  0.6933024771545233\n",
      "model:  dt 1000\n",
      " acc:  0.5860271115745568 f1score:  0.6087209535319967\n",
      "model:  gb 1000\n",
      " acc:  0.6892596454640251 f1score:  0.6795334448612061\n",
      "model:  lsvc 1000\n",
      " acc:  0.6522419186652764 f1score:  0.6399809780017214\n",
      "model:  svc 1000\n",
      " acc:  0.6386861313868614 f1score:  0.6264256040621544\n",
      "model:  log 1000\n",
      " acc:  0.6381647549530761 f1score:  0.6225801162852651\n"
     ]
    }
   ],
   "source": [
    "ks = [k] \n",
    "print(setting)\n",
    "for k in ks:\n",
    "    model_pipelines = [('rf',Pipeline([('clf', RandomForestClassifier(random_state=500))])),\n",
    "                       ('dt',Pipeline([('clf', DecisionTreeClassifier(random_state=500))])),\n",
    "                       ('gb',Pipeline([('clf', GradientBoostingClassifier(random_state=500))])),\n",
    "                       ('lsvc',Pipeline([('clf', LinearSVC(random_state=500))])),\n",
    "                       ('svc',Pipeline([('clf', SVC(random_state=500))])),\n",
    "                       ('log',Pipeline([('clf', LogisticRegression(random_state=500))])),\n",
    "                       ]\n",
    "    \n",
    "    for pipe in model_pipelines:\n",
    "        model_name = pipe[0]\n",
    "        pipeline = pipe[1]\n",
    "        print('model: ',model_name, k)\n",
    "\n",
    "        # Singlelabel classifier\n",
    "        model = OneVsRestClassifier(pipeline).fit(X_train_features_,train['primary_label'])\n",
    "        prediction = model.predict(X_test_features_)\n",
    "        acc = accuracy_score(test['primary_label'], prediction)\n",
    "        model_name = model_name+setting\n",
    "        acc_default[model_name][k] = acc\n",
    "        f1score = metrics.f1_score(test['primary_label'], prediction, average = 'weighted')\n",
    "        f1score_default[model_name][k] = f1score\n",
    "        print(' acc: ', acc, 'f1score: ', f1score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
