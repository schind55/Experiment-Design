{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of labels\n",
    "In this notebook, we perform an analysis to investigate the presence and distribution of data science activities in 470 Jupyter notebooks (performing a data science task). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os.path import dirname as up\n",
    "\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shutil import copyfile,copy\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "from collections import Counter\n",
    "\n",
    "from glob import glob\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "path = os.getcwd()\n",
    "features_path = os.path.join(path,'features','')\n",
    "results_path = os.path.join(path,'results','f_label_distribution','')\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "features_df = pd.read_pickle(features_path+'f_DASWOW.pkl')\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the features\n",
    "features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "#features_df.fillna(0,inplace=True)\n",
    "#features_df['execution_count'].fillna(-1,inplace=True) # if no execution count is available, set it to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['comment_only', 'data_exploration',\n",
    "       'data_preprocessing', 'evaluation','helper_functions', 'load_data','modelling','prediction', \n",
    "       'result_visualization', 'save_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = selected.groupby('filename')['modelling']\n",
    "selected = features_df.copy()\n",
    "print(len(set(selected['filename'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_nbs = []\n",
    "for key, item in ds:\n",
    "    if ds.get_group(key).values.sum()>=1:\n",
    "        modelling_nbs.append(key)\n",
    "        \n",
    "ds1 = selected[selected.filename.isin(modelling_nbs)] # modelling notebooks\n",
    "ds1.index = range(ds1.shape[0])\n",
    "ds1files = ds1.groupby('filename')\n",
    "\n",
    "ds2 = selected[~selected.filename.isin(modelling_nbs)] #non-modelling notebooks\n",
    "ds2.index = range(ds2.shape[0])\n",
    "ds2files = ds2.groupby('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that non-modelling notebooks have atleast one of the labels: data exploration or result visualization\n",
    "[key for key,item in ds2files if (ds2files.get_group(key)['data_exploration'].values.sum() + ds2files.get_group(key)['result_visualization'].values.sum()) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of labels throughout the modelling notebooks dataset\n",
    "labels_modelling = (ds1['primary_label'].value_counts()/ds1.shape[0])*100\n",
    "labels_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of labels throughout the non modelling notebooks dataset\n",
    "labels_nonmodelling = (ds2['primary_label'].value_counts()/ds2.shape[0])*100\n",
    "labels_nonmodelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['data_exploration','data_preprocessing','modelling','helper_functions',\n",
    "         'load_data','evaluation','result_visualization','prediction',\n",
    "         'comment_only','save_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,value,label=[],[],[]\n",
    "for o in order:\n",
    "    label.append(o)\n",
    "    model.append('modelling')\n",
    "    try:\n",
    "        value.append(labels_modelling[o])\n",
    "    except:\n",
    "        value.append(0)\n",
    "    \n",
    "for o in order:   \n",
    "    label.append(o)    \n",
    "    model.append('non-modelling')\n",
    "    try:\n",
    "        value.append(labels_nonmodelling[o])\n",
    "    except:\n",
    "        value.append(0)\n",
    "        \n",
    "dfs = pd.DataFrame(data={'type': model, \n",
    "                         'value': value,\n",
    "                        'label':label})\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(9,3)})\n",
    "g = sns.catplot(x = 'value', y='label', hue='type', data=dfs, kind='bar',order=order,legend=False)    \n",
    "title = \"distribution_of_labels_by_type_of_data_science_task\"\n",
    "for ax in g.axes.ravel():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_width(), '.2f'), \n",
    "                   (p.get_x() + p.get_width()+10,p.get_y()+.5), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "#ax.xaxis.grid(False)\n",
    "ax.set_xlabel(\"% of code cells in the dataset\",fontsize=12,fontweight='bold')\n",
    "ax.set_ylabel(\"\",fontsize=12,fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "#axes=plt.gca()\n",
    "ax.set(xlim=(0, 100))\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path+title+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly occuring sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq = []\n",
    "for idx,row in selected.iterrows():\n",
    "    seq = []\n",
    "    for l in labels:\n",
    "        if row[l]==1:\n",
    "            seq.append(l)\n",
    "    all_seq.append(tuple(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top,top1,top2,top3,top4,top5 = [],[],[],[],[],[]\n",
    "\n",
    "topsequences = Counter(all_seq).most_common()\n",
    "for each in topsequences:\n",
    "    top.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==1:\n",
    "        if len(top1)<5:\n",
    "            top1.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==2:\n",
    "        if len(top2)<5:\n",
    "            top2.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==3:\n",
    "        if len(top3)<5:\n",
    "            top3.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==4:\n",
    "        if len(top4)<5:\n",
    "            top4.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==5:\n",
    "        if len(top5)<5:\n",
    "            top5.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsets = powerset(labels)\n",
    "labelsets_dict = {}\n",
    "for labelset in labelsets:\n",
    "    count = 0\n",
    "    for seq in all_seq:\n",
    "        if (all(s in list(seq) for s in labelset)):\n",
    "            count += 1\n",
    "    labelsets_dict[labelset] = round((count/selected.shape[0])*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(labelsets_dict.items(), key=lambda item: item[1], reverse=True) if len(k)==3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of number of labels throughout the dataset\n",
    "c = Counter(features_df[labels].sum(axis=1).values)\n",
    "step_dict = pd.Series(data=c, index=c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of primary labels throughout the dataset\n",
    "d = pd.DataFrame([dict(step_dict)]).T\n",
    "d.columns = ['count']\n",
    "d['step'] = d.index.astype(str)\n",
    "d['percentage'] = ((step_dict/selected.shape[0])*100).values.round(2) \n",
    "d.index = range(d.shape[0])\n",
    "truedistdf = d.copy()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(d,col,title,xlabel,ylabel,order):\n",
    "    plt.figure(figsize=(9,3))\n",
    "\n",
    "    ax = sns.barplot(x=col,y='step',data=d,color=\"skyblue\")\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_width(), '.2f'), \n",
    "                   (p.get_x() + p.get_width()+1.2, p.get_y()+0.5), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 0), \n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.set_xlabel(xlabel,fontsize=12,fontweight='bold')\n",
    "    ax.set_ylabel(ylabel,fontsize=12,fontweight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    axes=plt.gca()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(results_path+title+\".png\",dpi=300)\n",
    "    plt.savefig(results_path+title+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(d,'percentage',\n",
    "                  \"no_of_data_science_activities_in_the_dataset\",\"% of code cells in the dataset\",\"no of labels\",\n",
    "                  d.step.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def get_truth(inp, op, cut):\n",
    "    return op(inp, cut)\n",
    "\n",
    "def get_n_label_notebooks(df,labels,n,op):\n",
    "    filegroup = df.groupby(['filename'])\n",
    "    single_label_nbs = [k for k,v in filegroup if get_truth(max(v[labels].sum(axis=1).values),op,n)]\n",
    "    \n",
    "    print(\"No. of single label notebooks in the dateset: {0}.\".format(len(single_label_nbs)),\n",
    "          \"% of single label notebooks in the dataset: {0:.2f}\".format(len(single_label_nbs)/len(set(df.filename.values))*100))\n",
    "\n",
    "    #### update the dataframe with notebooks that have only one labels per cell throughout\n",
    "    single_label_df = df[df.filename.isin(single_label_nbs)]\n",
    "    return single_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the notebooks have only one label per cell throughout\n",
    "single_label_df = get_n_label_notebooks(selected,labels,1,operator.eq)\n",
    "print(single_label_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of primary labels throughout the dataset\n",
    "step_dict = selected['primary_label'].value_counts()\n",
    "d = pd.DataFrame([dict(step_dict)]).T\n",
    "d.columns = ['count']\n",
    "d['step'] = d.index\n",
    "d['percentage'] = ((step_dict/selected.shape[0])*100).values.round(2) \n",
    "d.index = range(d.shape[0])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(d,'percentage',\n",
    "                  \"primary_label_data_science_activities_in_the_dataset\",\n",
    "                  \"% of code cells in the dataset\",\"\",\n",
    "                 step_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_multi_dict = selected[labels].sum(axis=0)\n",
    "multi_dict = sorted(unsorted_multi_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "d = pd.DataFrame([dict(multi_dict)]).T\n",
    "d.columns = ['count']\n",
    "d['step'] = d.index\n",
    "### distribution of labels throughout the dataset\n",
    "d['percentage'] = d['count'].div(selected.shape[0]) #d['count'].sum()\n",
    "d['percentage'] = d['percentage'].apply(lambda x: x*100).round(2)\n",
    "d.index = range(d.shape[0])\n",
    "dtrue = d.copy()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(d,'percentage',\n",
    "                  \"all_labels_data_science_activities_in_the_dataset\",\n",
    "                  \"% of code cells in the dataset\",\"\",\n",
    "                 step_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the distribution of code and comment using primary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_linesofcode = len(selected['text'].sum(axis=0))\n",
    "tot_linesofcomments = len(selected['comment'].sum(axis=0))\n",
    "print(\"Total lines of code: \", tot_linesofcode, \" Total lines of comment: \", tot_linesofcomments)\n",
    "for l in labels:\n",
    "    print(l, \" -> \", \"linesofcode {0:.2f}\".format((len(selected[selected['primary_label']==l]['text'].sum(axis=0))/tot_linesofcode)*100), \"linesofcomment {0:.2f}\".format((len(selected[selected['primary_label']==l]['comment'].sum(axis=0))/tot_linesofcomments)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_code = 0\n",
    "code = 0\n",
    "lines_of_code = {}\n",
    "    \n",
    "for idx,row in selected.iterrows():\n",
    "        code += len(row['text'])\n",
    "        if row['primary_label'] == 'data_exploration':\n",
    "            prep_code += len(row['text'])\n",
    "print(code,prep_code,prep_code/code*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the distribution of lines of code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features_df[features_df.cell_type=='code'].copy()\n",
    "#lines of code\n",
    "len(list(itertools.chain(*list(df['text'].values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines of comment\n",
    "len(list(itertools.chain(*list(df['comment'].values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_length = []\n",
    "for cell in df['text'].values:\n",
    "    cell_length.append(len(cell))\n",
    "c = Counter(cell_length)\n",
    "loc_dict = {'0':0,'1-5':0,'6-10':0,'11-20':0,'>20':0}\n",
    "for k,v in c.items():\n",
    "    if k==0:\n",
    "        loc_dict['0'] = loc_dict['0']+v\n",
    "    elif (k>=1) and (k<=5):\n",
    "        loc_dict['1-5'] = loc_dict['1-5']+v\n",
    "    elif (k>=6) and (k<=10):\n",
    "        loc_dict['6-10'] = loc_dict['6-10']+v\n",
    "    elif (k>=11) and (k<=20):\n",
    "        loc_dict['11-20'] = loc_dict['11-20']+v\n",
    "    elif k>20:\n",
    "        loc_dict['>20'] = loc_dict['>20']+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame([loc_dict]).T\n",
    "d.columns = ['count']\n",
    "d['lines of code'] = d.index\n",
    "d['percentage'] = d['count']/d['count'].sum()\n",
    "d['percentage'] = d['percentage'].round(2)\n",
    "d.index = range(d.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "\n",
    "ax = sns.barplot(x='percentage',y='lines of code',data=d,color=\"skyblue\",order=loc_dict.keys())\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                   (p.get_x() + p.get_width()+0.02, p.get_y()+0.7), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "title = \"updated no. of lines of code per cell\"\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_xlabel(\"% of cells in the dataset\",fontsize=12,fontweight='bold')\n",
    "ax.set_ylabel(\"no. of lines of code per cell\",fontsize=12,fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.set(xlim=(0, 0.8))\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "axes=plt.gca()\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path+\"codelinespercell.eps\",format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the distribution of predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = pd.read_pickle(os.path.join(path,'results','multi-label-classification-model-02-20-2021','')+'prediction_multi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_multi_dict = dfpred[labels].sum(axis=0)\n",
    "multi_dict = sorted(unsorted_multi_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "d = pd.DataFrame([dict(multi_dict)]).T\n",
    "d.columns = ['count']\n",
    "d['step'] = d.index\n",
    "### distribution of labels throughout the dataset\n",
    "d['percentage'] = d['count'].div(dfpred.shape[0])\n",
    "d['percentage'] = d['percentage'].apply(lambda x: x*100).round(2)\n",
    "d.index = range(d.shape[0])\n",
    "dpred = d.copy()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(d,'percentage',\n",
    "                  \"predicted_label_data_science_activities_in_the_dataset\",\n",
    "                  \"% of code cells in the dataset\",\"\",\n",
    "                 d.step.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truevals,predvals = [],[]\n",
    "for l in labels:\n",
    "    predvals.append(dpred[dpred['step']==l]['percentage'].values[0])\n",
    "    truevals.append(dtrue[dtrue['step']==l]['percentage'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "from scipy.stats import spearmanr,kendalltau\n",
    "    \n",
    "# calculate kendall's correlation\n",
    "coef, p = kendalltau(truevals, predvals)\n",
    "print('Kendall correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distribution of number of labels throughout the dataset\n",
    "c = Counter(dfpred[labels].sum(axis=1).values)\n",
    "step_dict = pd.Series(data=c, index=c.keys())\n",
    "\n",
    "### distribution of primary labels throughout the dataset\n",
    "d = pd.DataFrame([dict(step_dict)]).T\n",
    "d.columns = ['count']\n",
    "d['step'] = d.index.astype(str)\n",
    "d['percentage'] = ((step_dict/dfpred.shape[0])*100).values.round(2) \n",
    "d.index = range(d.shape[0])\n",
    "preddistdf = d.copy()\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(d,'percentage',\n",
    "                  \"no_of_data_science_activities_in_the_predicted_dataset\",\"% of code cells in the dataset\",\"no of labels\",\n",
    "                  d.step.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truevals,predvals = [],[]\n",
    "for l in [0,1,2,3,4,5]:\n",
    "    try:\n",
    "        truevals.append(truedistdf[truedistdf['step']==str(l)]['percentage'].values[0])\n",
    "    except:\n",
    "        truevals.append(0)\n",
    "    try:\n",
    "        predvals.append(preddistdf[preddistdf['step']==str(l)]['percentage'].values[0])\n",
    "    except:\n",
    "        predvals.append(0)\n",
    "print(truevals)\n",
    "print(predvals)\n",
    "# calculate kendall's correlation\n",
    "coef, p = kendalltau(truevals, predvals)\n",
    "print('Kendall correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq = []\n",
    "for idx,row in dfpred.iterrows():\n",
    "    seq = []\n",
    "    for l in labels:\n",
    "        if row[l]==1:\n",
    "            seq.append(l)\n",
    "    all_seq.append(tuple(seq))\n",
    "    \n",
    "top,top1,top2,top3,top4,top5 = [],[],[],[],[],[]\n",
    "topsequences = Counter(all_seq).most_common()\n",
    "for each in topsequences:\n",
    "    top.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==1:\n",
    "        if len(top1)<5:\n",
    "            top1.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==2:\n",
    "        if len(top2)<5:\n",
    "            top2.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==3:\n",
    "        if len(top3)<5:\n",
    "            top3.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==4:\n",
    "        if len(top4)<5:\n",
    "            top4.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "    if len(each[0])==5:\n",
    "        if len(top5)<5:\n",
    "            top5.append((each[0],each[1],round((each[1]/features_df.shape[0])*100,2)))\n",
    "            \n",
    "labelsets = powerset(labels)\n",
    "labelsets_dict = {}\n",
    "for labelset in labelsets:\n",
    "    count = 0\n",
    "    for seq in all_seq:\n",
    "        if (all(s in list(seq) for s in labelset)):\n",
    "            count += 1\n",
    "    labelsets_dict[labelset] = round((count/selected.shape[0])*100,2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(labelsets_dict.items(), key=lambda item: item[1], reverse=True) if len(k)==1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of number of lines of code per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['helper_functions','load_data',\n",
    "              'data_preprocessing','data_exploration',\n",
    "              'modelling','evaluation','prediction',\n",
    "              'result_visualization','save_results',\n",
    "              'comment_only']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "ax = sns.boxplot(x=\"linesofcode\", y=\"primary_label\", \n",
    "            showmeans=True, meanprops={\"marker\": \"+\",\n",
    "                       \"markeredgecolor\": \"black\",\n",
    "                       \"markersize\": \"10\"}, \n",
    "            data=features_df[features_df['cell_type']=='code'],\n",
    "            showfliers=True,order=labels, \n",
    "           flierprops = dict(markerfacecolor='0.75', markersize=5,\n",
    "              linestyle='none'))\n",
    "#sns.despine(offset=10, trim=False)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.ylabel(\"data science step (primary_label)\", size=12)\n",
    "plt.xlabel(\"number of lines of code\", size=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path+\"loc_log.eps\",format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
