{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Multiclass classification\n",
    "\n",
    "This notebook tries to model the dataset using Multilabel Multiclass classification.\n",
    "- all the labels except primary_label is used as multi label input (note: primary_label is already taken into account in this set). \n",
    "- in this notebook, we first try different classifiers (with default parameters) and compare their accuracy performance. We then evaluate the performance of the best performing classifier using GridSearchCV.\n",
    "- before classification, we preprocess the data using class:classifier. features are representing using tfidf method and chi2 method is used for feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Classifiers.Classifiers import Classifiers\n",
    "from classes.Classifiers.ClassBalancer import ClassBalancer\n",
    "from classes.Classifiers.FeatureSelector import FeatureSelector\n",
    "from classes.Classifiers.ModelComparison import ModelComparison\n",
    "from classes.Utils.PandasUtils import PandasUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from glob import glob \n",
    "\n",
    "#os related operations\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#data structures\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from itertools import islice\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_classif \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve  \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC,SVC #multiclass\n",
    "from sklearn.linear_model import LogisticRegression #multiclass\n",
    "from sklearn.ensemble import RandomForestClassifier #multilabel, multioutput\n",
    "from sklearn.tree import DecisionTreeClassifier  #multilabel, multioutput\n",
    "from sklearn.ensemble import GradientBoostingClassifier #multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "results_path = os.path.join(path,'results','multi-label-classification','')\n",
    "features_path = os.path.join(path,'features','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are stored in a dict\n",
    "subset_acc_default = {}\n",
    "hamming_loss_default = {}\n",
    "f1score_default = {}\n",
    "jaccard_default = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='code'\n",
    "setting = '_'+content+'_'+'scaled'\n",
    "settings = [m+setting for m in ['rf','dt','gb','lsvc','svc','log'] ]\n",
    "\n",
    "for model_name in settings:\n",
    "    jaccard_default[model_name] = {}\n",
    "    f1score_default[model_name] = {}\n",
    "    subset_acc_default[model_name] = {}\n",
    "    hamming_loss_default[model_name] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_pickle(features_path+'test_features.pkl')\n",
    "train_features = pd.read_pickle(features_path+'train_features.pkl')\n",
    "validation_features = pd.read_pickle(features_path+'validation_features.pkl')\n",
    "\n",
    "train_features.index = range(train_features.shape[0])\n",
    "validation_features.index = range(validation_features.shape[0])\n",
    "test_features.index = range(test_features.shape[0])\n",
    "\n",
    "train_features.fillna(0,inplace=True)\n",
    "test_features.fillna(0,inplace=True)\n",
    "validation_features.fillna(0,inplace=True)\n",
    "\n",
    "print(train_features.shape)\n",
    "print(validation_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['helper_functions','load_data',\n",
    "              'data_preprocessing','data_exploration',\n",
    "              'modelling','evaluation','prediction',\n",
    "              'result_visualization','save_results',\n",
    "              'comment_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training + validation dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "#df= df.add(features)\n",
    "df = df.add(train_features)\n",
    "df = df.add(validation_features)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total notebook in training+validation: \",len(set(df.filename.values)))\n",
    "print(\"Total notebook in test: \",len(set(test_features.filename.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Setup and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorisation of features \n",
    "*if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df):\n",
    "    cat = pd.Categorical(df['primary_label'].values, categories=labels)\n",
    "    y, uniques = pd.factorize(cat)\n",
    "    df['primary_label'] = np.asarray(y)\n",
    "    \n",
    "    cat = pd.Categorical(df['filename'].values)\n",
    "    files, uniques = pd.factorize(cat)\n",
    "    df['filename'] = np.asarray(files)\n",
    "    \n",
    "    return df,files,cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df = temp_df.add(df.copy())\n",
    "temp_df = temp_df.add(test_features.copy())\n",
    "temp_df.index = range(temp_df.shape[0])\n",
    "temp_df,cat,files = categorize(temp_df)\n",
    "\n",
    "df1 = temp_df[0:df.shape[0]].copy()\n",
    "df1.index = range(df1.shape[0])\n",
    "print(df1.primary_label.unique())\n",
    "print(df1.shape)\n",
    "\n",
    "df2 = temp_df[df.shape[0]:].copy()\n",
    "df2.index = range(df2.shape[0])\n",
    "print(df2.primary_label.unique())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pre-defined split of validation set for crossfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = []\n",
    "for i in range(0,train_features.shape[0]):\n",
    "    test_fold.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = {}\n",
    "t_fold = 1\n",
    "count = 1\n",
    "for each in set(validation_features.filename.values):\n",
    "    folds[each] = t_fold \n",
    "    if count%10 == 0:\n",
    "        t_fold += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in validation_features.iterrows():\n",
    "    test_fold.append(folds[row['filename']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "print(set(test_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the classifier and indicate conditions to restrict the dataframe we will be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifiers(df1,labels) #provide the training set and labels\n",
    "conditions = (df1.cell_type == 'code')\n",
    "model.apply_conditions_to_dataframe(conditions) #a restricted dataframe is created\n",
    "\n",
    "(train,test,indices_train,indices_test)=model.test_train_data_set(df2) #provide the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (Representation and Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'text','comment'\n",
    "# extend: 'code_line_before','code_line_after', 'markdown_heading', 'packages_info'\n",
    "features = ['text']\n",
    "train,test = model.set_lexical_features(features)\n",
    "#same df is modified and contains our new feature column *new_text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply preprocessing to the text feature\n",
    "many custom preprocessing functions are available inside the class Preprocessing, check out!\n",
    "\n",
    "here, we will use code_text_processing function to process our lexical feature *new_text* we created previously\n",
    "\n",
    "processed text will be available in the feature column *text_processed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = model.preprocessing('new_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tfidf and chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text features\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),use_idf=True,max_df=0.2,min_df=2,stop_words='english') \n",
    "X_train,X_test,tfidf = model.vectorization(tfidf)\n",
    "print(X_train.shape)\n",
    "\n",
    "#use feature selection if necessary \n",
    "k = 2000 #k lexical features to be retained\n",
    "X_train_features,X_test_features,selector = model.feature_selection(chi2,k,train[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = tfidf.get_feature_names()\n",
    "print(\"tfidf features: \",len(tfidf_features))\n",
    "print(\"before feature selection: \",len(selector.get_support()))\n",
    "selected_features = selector.get_support()\n",
    "text_features = [tfidf_features[i] for i in range(len(tfidf.get_feature_names())) if selected_features[i]==True]\n",
    "print(\"after feature selection: \",len(text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following set of features are represented in numerical form\n",
    "#metric features are: ['linesofcomment','linesofcode','variable_count','function_count']\n",
    "#extended features are: ['filename','cell_number','execution_count','text/plain' , 'image/png', 'text/html', 'execute_result', 'display_data', 'stream', 'error']\n",
    "\n",
    "stat_features = ['linesofcomment','linesofcode','variable_count','function_count']\n",
    "X_train_features_,X_test_features_ = model.set_statistical_features(stat_features,X_train_features,X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Together, text_features and stat_features form the original feature vector of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = tfidf_features+stat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the features \n",
    "since our statistical features are scaled in a different way to our text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_features_ = ss.fit_transform(X_train_features_)\n",
    "X_test_features_ = ss.transform(X_test_features_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare classifiers\n",
    "*default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ks = [k] \n",
    "for k in ks:\n",
    "    print(setting)\n",
    "    model_pipelines = [('rf',Pipeline([('clf', RandomForestClassifier(random_state=500))])),\n",
    "                       ('dt',Pipeline([('clf', DecisionTreeClassifier(random_state=500))])),\n",
    "                       ('gb',Pipeline([('clf', GradientBoostingClassifier(random_state=500))])),\n",
    "                       ('lsvc',Pipeline([('clf', LinearSVC(random_state=500))])),\n",
    "                       ('svc',Pipeline([('clf', SVC(random_state=500))])),\n",
    "                       ('log',Pipeline([('clf', LogisticRegression(random_state=500))])),\n",
    "                       ]\n",
    "    \n",
    "    for pipe in model_pipelines:\n",
    "        model_name = pipe[0]\n",
    "        pipeline = pipe[1]\n",
    "        print('model: ',model_name, k)\n",
    "\n",
    "        # Multilabel classifier\n",
    "        model = OneVsRestClassifier(pipeline).fit(X_train_features_,train[labels])\n",
    "        prediction = model.predict(X_test_features_)\n",
    "        subset_acc = accuracy_score(test[labels], prediction)\n",
    "        model_name = model_name+setting\n",
    "        #subset_acc_default[model_name][k] = subset_acc\n",
    "        hamming = metrics.hamming_loss(test[labels], prediction)\n",
    "        #hamming_loss_default[model_name][k] = hamming\n",
    "        jaccard = metrics.jaccard_score(test[labels], prediction,average=\"weighted\")\n",
    "        f1score = metrics.f1_score(test[labels], prediction, average = 'weighted')\n",
    "        #jaccard_default[model_name][k] = jaccard\n",
    "        #f1score_default[model_name][k] = f1score\n",
    "        print(' subset acc: ',subset_acc)\n",
    "        print(' Hamming: ',hamming,' jaccard: ', jaccard, 'f1score: ', f1score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.from_dict(f1score_default).to_pickle(results_path+'subset_acc_default.pkl')\n",
    "#pd.DataFrame.from_dict(acc_default).to_pickle(results_path+'hamming_loss_default.pkl')\n",
    "#pd.DataFrame.from_dict(f1score_default).to_pickle(results_path+'f1score_default.pkl')\n",
    "#pd.DataFrame.from_dict(acc_default).to_pickle(results_path+'jaccard_default.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(f1score_default).max().max()\n",
    "\n",
    "for k,v in f1score_default.items():\n",
    "    for key,value in v.items():\n",
    "        if value >= pd.DataFrame.from_dict(f1score_default).max().max():\n",
    "            print(k,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1f = pd.DataFrame.from_dict(f1score_default)\n",
    "saf = pd.DataFrame.from_dict(subset_acc_default)\n",
    "hlf = pd.DataFrame.from_dict(hamming_loss_default)\n",
    "jcf = pd.DataFrame.from_dict(jaccard_default)\n",
    "\n",
    "f1score_dict, sacc_dict, hamming_dict, jaccard_dict = {},{},{},{}\n",
    "for col in f1f.T[2000].index.values:\n",
    "    if '_scaled' in col:\n",
    "        f1score_dict[col] = round(f1f[col][1000],3)\n",
    "        sacc_dict[col] = round(saf[col][1000],3)\n",
    "        hamming_dict[col] = round(hlf[col][1000],3)\n",
    "        jaccard_dict[col] = round(jcf[col][1000],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slabel = pd.DataFrame({'model':f1score_dict.keys(),\n",
    "                       'subset accuracy':sacc_dict.values(),\n",
    "                      'hamming loss':hamming_dict.values(),\n",
    "                      'jaccard score':jaccard_dict.values(),\n",
    "                       'f1 score':f1score_dict.values()})\n",
    "#slabel.index = slabel['model']\n",
    "#slabel.to_pickle(results_path+'multi_label_results.pkl')\n",
    "slabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Classifiers: Best parameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = {}\n",
    "subset_accuracy_all = {}\n",
    "hamming_loss_all = {}\n",
    "jaccard_sim_all = {}\n",
    "estimators_all = []\n",
    "f1_score_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.add(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "\n",
    "parameters = {'rf':{}}\n",
    "parameters['rf']['estimator__rf__criterion'] = ['gini','entropy']\n",
    "parameters['rf']['estimator__rf__n_estimators'] = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "parameters['rf']['estimator__rf__class_weight']= ['balanced', 'balanced_subsample']\n",
    "\n",
    "k = 2000\n",
    "print(\"******\",k,\"*******\")\n",
    "model_pipelines = [('rf',Pipeline([('rf', RandomForestClassifier(random_state=500))]))]\n",
    "    \n",
    "for pipe in model_pipelines:\n",
    "    model_name = pipe[0]+'['+content+']'\n",
    "    pipeline = pipe[1]\n",
    "\n",
    "    print('model: ',model_name)\n",
    "    results_all[model_name] = {}\n",
    "    # Multilabel classifier    \n",
    "    CV = GridSearchCV(OneVsRestClassifier(pipeline), parameters[pipe[0]], scoring = 'accuracy', n_jobs= 1,cv=ps)\n",
    "    CV.fit(X_train_features_,train[labels])\n",
    "    print(CV.best_estimator_.classes_)\n",
    "    estimators_all.add(CV)\n",
    "    prediction_multi = CV.predict(X_test_features_)\n",
    "    #evaluate\n",
    "    subset_acc = accuracy_score(test[labels], prediction_multi)\n",
    "    hamming = metrics.hamming_loss(test[labels], prediction_multi)\n",
    "    jaccard = metrics.jaccard_score(test[labels], prediction_multi, average = 'weighted')\n",
    "    f1score = metrics.f1_score(test[labels], prediction_multi, average = 'weighted')\n",
    "    print(' subset acc: ',subset_acc)\n",
    "    print(' Hamming: ',hamming,' jaccard: ', jaccard)\n",
    "    print(' f1score: ',f1score)\n",
    "    subset_accuracy_all[model_name+str(k)] = subset_acc\n",
    "    hamming_loss_all[model_name+str(k)] = hamming\n",
    "    jaccard_sim_all[model_name+str(k)] = jaccard   \n",
    "    f1_score_all[model_name+str(k)] = f1score        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[labels], prediction_multi, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsums = test[labels].iloc[:,0:].sum(axis=1)\n",
    "true=rowsums.value_counts()\n",
    "print(true)\n",
    "rowsums = pd.DataFrame(prediction_multi).iloc[:,0:].sum(axis=1)\n",
    "pred=rowsums.value_counts()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, types, values = [], [], []\n",
    "for idx in pred.index.values:\n",
    "    try:\n",
    "        values.add(pred.iloc[idx])\n",
    "        types.add('pred')\n",
    "        idxs.add(idx)\n",
    "    except:\n",
    "        values.add(0)\n",
    "        types.add('pred')\n",
    "        idxs.add(idx)\n",
    "    try:\n",
    "        values.add(true.iloc[idx])\n",
    "        types.add('true')\n",
    "        idxs.add(idx)\n",
    "    except:\n",
    "        values.add(0)\n",
    "        types.add('true')\n",
    "        idxs.add(idx)\n",
    "plot = pd.DataFrame({'no of labels':idxs,'% distribution':values,'type':types})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(9,3)})\n",
    "g = sns.catplot(y = '% distribution', x ='no of labels', hue='type', data=plot, kind='bar',legend=False)    \n",
    "title = \"Labels per cell\"\n",
    "for ax in g.axes.ravel():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width()-0.2,p.get_y()+p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points',\n",
    "                   fontsize=10)\n",
    "#ax.xaxis.grid(False)\n",
    "ax.set_xlabel(\"% of code cells in the dataset\",fontsize=12,fontweight='bold')\n",
    "ax.set_ylabel(\"\",fontsize=12,fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "#axes=plt.gca()\n",
    "ax.set(ylim=(0, 1750))\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path+'labeldistribution.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = pd.DataFrame(prediction_multi,columns=labels)\n",
    "dfpred['filename'] = df2.filename\n",
    "dfpred.to_pickle(results_path+'prediction_multi.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
